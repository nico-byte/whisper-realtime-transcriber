import numpy as np
import asyncio
import sys
import typing as t
from copy import deepcopy

try:
    import sounddevice as sd
except OSError as e:
    print(e)
    print("If `GLIBCXX_x.x.x' not found, try installing it with: conda install -c conda-forge libstdcxx-ng=12")
    sys.exit()


class InputStreamGenerator:
    """
    Loading and using the InputStreamGenerator.

    Parameters
    ----------
    samplerate : int
        The specified samplerate of the audio data. (default is 16000)
    blocksize : int
        The size of each individual audio chunk. (default is 4000)
    adjustment_time : int
        The adjustment_time for setting the silence threshold. (default is 5)
    min_chunks : int
        The minimum number of chunks to be generated, before feeding it into the asr model. (default is 6)
    continuous : bool
        Whether to generate audio data conituously or not. (default is True)
    memory_safe: bool
        Whether to pause the generation audio data during the inference of the asr model or not. (default is True)
    verbose : bool
        Whether to print the additional information to the console or not. (default is True)

    Attributes
    ----------
    samplerate : int
        The samplerate of the generated audio data.
    temp_ndarray : np.ndarray
        Where the generated audio data is stored.
    data_ready_event : asyncio.Event
        Boolean to tell the InputStreamGenerator, that the asr model is busy or not.
    continuous : bool
        Where the boolean to decide if the InputStreamGenerator runs continuously.
    memory_safe: bool
        If True, InputStreamGenerator will pause the collection of audio data during model inference.
    verbose : bool
        Where the boolean to decide to print the model outputs is stored.

    Methods
    -------
    process_audio()
        Processes the incoming audio data.
    """

    def __init__(
        self,
        samplerate: int = 16000,
        blocksize: int = 4000,
        adjustment_time: int = 5,
        min_chunks: int = 6,
        phrase_delta: int = 1.5,
        continuous: bool = True,
        memory_safe: bool = True,
        verbose: bool = True,
    ):
        self.samplerate = samplerate
        self._blocksize = blocksize
        self._adjustment_time = adjustment_time
        self._min_chunks = min_chunks
        self.continuous = continuous
        self.memory_safe = memory_safe
        self.verbose = verbose

        self._global_ndarray: np.ndarray = None
        self.temp_ndarray: np.ndarray = None

        self._phrase_delta_blocks = (samplerate // blocksize) * phrase_delta
        self.complete_phrase_event = asyncio.Event()

        self._silence_threshold = None

        self.data_ready_event = asyncio.Event()

    async def _generate(self) -> t.AsyncGenerator:
        """
        Generate audio chunks of size of the blocksize and yield them.
        """
        q_in = asyncio.Queue()
        loop = asyncio.get_event_loop()

        def callback(in_data, _, __, state):
            loop.call_soon_threadsafe(q_in.put_nowait, (in_data.copy(), state))

        stream = sd.InputStream(
            samplerate=self.samplerate,
            channels=1,
            dtype="int16",
            blocksize=self._blocksize,
            callback=callback,
        )
        with stream:
            while True:
                indata, status = await q_in.get()
                yield indata, status

    async def process_audio(self) -> None:
        """
        Continuously processes audio input, detects significant speech segments, and prepares them for transcription.

        This method processes incoming audio data in real-time, filtering out segments that are mostly silence
        and concatenating buffers to form meaningful audio chunks. It uses a silence threshold to discard
        buffers that do not contain significant speech and prepares the audio data for transcription when enough
        data is collected.

        Workflow:
            1. **Set Silence Threshold**: If the silence threshold (`_silence_threshold`) is not set, it initializes
               it by calling `_set_silence_threshold()`.

            2. **Listening Loop**: It enters an asynchronous loop where it continuously processes audio buffers
               generated by the `_generate()` method.

            3. **Silence Detection**: Each audio buffer (`indata`) is flattened and its absolute values are analyzed
               to detect silence:
                - Buffers with silence below the threshold are discarded unless there's ongoing data in `self._global_ndarray`.
                - If the system is in memory-safe mode and `data_ready_event` is set, the buffer is also discarded.

            4. **Buffer Concatenation**:
                - If there is ongoing data in `self._global_ndarray`, the current buffer is concatenated with it.
                - If not, the current buffer initializes `self._global_ndarray`.

            5. **End-of-Buffer Check**: The method checks the end of the buffer to determine if it contains significant speech.
                - If it does, it continues listening for more data.
                - If not and the collected data exceeds a minimum chunk size (`_min_chunks`), the buffer is prepared for transcription:
                    - The data is copied to `self.temp_ndarray`, flattened, and normalized.
                    - `self._global_ndarray` is reset to `None`.
                    - The `data_ready_event` is set to signal that transcription can begin.

            6. **Continuous Mode Check**: If the `continuous` mode is disabled, the method returns after processing a
               sufficient chunk of audio data; otherwise, it continues processing.

        Returns:
            None: The method runs indefinitely in continuous mode, processing audio input and preparing it for transcription.
            If `continuous` mode is disabled, it returns after the first significant chunk of audio is processed.
        """

        if self._silence_threshold is None:
            await self._set_silence_threshold()

        print("Listening...")
        empty_blocks = 0

        async for indata, _ in self._generate():
            indata_flattened: np.ndarray = abs(indata.flatten())
            if self._global_ndarray is not None and np.percentile(indata_flattened, 10) <= self._silence_threshold:
                empty_blocks += 1
                if empty_blocks >= self._phrase_delta_blocks:
                    empty_blocks = 0
                    self.complete_phrase_event.set()
                    await self._send_audio() if not self.data_ready_event.is_set() else None
                    if not self.continuous:
                        return None
                continue

            # discard buffers that contain mostly silence
            if (np.percentile(indata_flattened, 10) <= self._silence_threshold and self._global_ndarray is None) or (
                self.memory_safe and self.data_ready_event.is_set()
            ):
                continue

            # concatenate buffers
            if self._global_ndarray is not None:
                self._global_ndarray = np.concatenate((self._global_ndarray, indata), dtype="int16")
            else:
                self._global_ndarray = indata

            empty_blocks = 0

            if (np.percentile(indata_flattened[-100:-1], 10) > self._silence_threshold) or self.data_ready_event.is_set():
                continue

            # Process the global ndarray if the required chunks are met
            if len(self._global_ndarray) / self._blocksize >= self._min_chunks:
                await self._send_audio()

    async def _send_audio(self):
        self.temp_ndarray = deepcopy(self._global_ndarray)
        self._global_ndarray = None
        self.data_ready_event.set()

    async def _set_silence_threshold(self) -> None:
        """
        Dynamically sets the silence threshold for audio processing based on the average loudness of initial audio blocks.

        This method listens to the initial few seconds of audio input to determine an appropriate silence threshold.
        It calculates the loudness of the audio blocks during this period and sets the silence threshold to the 20th percentile
        of the observed loudness values. This threshold is used later to differentiate between significant audio and silence.

        Workflow:
            1. **Initialize Variables**:
                - `blocks_processed`: Tracks the number of audio blocks processed.
                - `loudness_values`: Stores the mean loudness of each audio block for later analysis.

            2. **Audio Block Processing**:
                - The method enters an asynchronous loop where it continuously processes audio blocks generated by `_generate()`.
                - For each block, it calculates the mean loudness and adds it to `loudness_values`.

            3. **Threshold Adjustment**:
                - The method continues processing until it has processed a sufficient number of blocks (determined by `_adjustment_time`).
                - Once enough blocks have been processed, it calculates the silence threshold as the 20th percentile of the collected loudness values.
                - This ensures that the threshold is set to a level that filters out most of the silence while capturing meaningful audio.

            4. **Verbose Output**:
                - If `verbose` mode is enabled, the method prints the newly set silence threshold for debugging or informational purposes.

        Returns:
            None: The method sets the silence threshold and then exits. It does not return any value.
        """

        blocks_processed: int = 0
        loudness_values: list = []

        async for indata, _ in self._generate():
            blocks_processed += 1
            indata_flattened: np.ndarray = abs(indata.flatten())

            # Compute loudness over first few seconds to adjust silence threshold
            loudness_values.append(np.mean(indata_flattened))

            # Stop recording after ADJUSTMENT_TIME seconds
            if blocks_processed >= self._adjustment_time * (self.samplerate / self._blocksize):
                self._silence_threshold = float(np.percentile(loudness_values, 10))
                break

        if self.verbose:
            print(f"Set SILENCE_THRESHOLD to {self._silence_threshold}\n")
